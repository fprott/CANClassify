{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess Data for CANClassify"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "# important! this will break plots on some windows running jupyter notebook - jupyter lab must be used instead\n",
    "from main import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_lib.list_local_devices()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare the data\n",
    "\n",
    "Download the data from cyverse and place the data into the data/ folder.\n",
    "\n",
    "The toyota vehicle (vehicle ids 2T3Y1RFV8KC014025, 2T3MWRFVXLW056972) has labeled radar data.\n",
    "\n",
    "The honda vehicle (vehicle id 5FNYF6H05HB089022) has some labeled data, but no labeled radar data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_paths_toyota = [\n",
    "    \"data/2020-08-13-13-26-45_2T3Y1RFV8KC014025_CAN_Messages.csv\",\n",
    "    \"data/2020-09-04-10-07-55_2T3Y1RFV8KC014025_CAN_Messages.csv\",\n",
    "    \"data/2020-10-17-10-34-29_2T3MWRFVXLW056972_CAN_Messages.csv\",\n",
    "    \"data/2020-10-17-13-40-39_2T3MWRFVXLW056972_CAN_Messages.csv\",\n",
    "    \"data/2020-11-05-09-10-00_2T3Y1RFV8KC014025_CAN_Messages.csv\"\n",
    "]\n",
    "\n",
    "csv_paths_honda = [\n",
    "    \"data/2020-09-18-11-36-54_5FNYF6H05HB089022_CAN_Messages.csv\",\n",
    "    \"data/2020-11-05-08-39-21_5FNYF6H05HB089022_CAN_Messages.csv\"\n",
    "]\n",
    "\n",
    "# we don't have a csv with Nissan Leaf 2018 data yet. We need to get this\n",
    "\n",
    "csv_paths_nissan = [\n",
    "    \"data/nissan_3_test.csv\"\n",
    "    #\"data/2021-11-11-00-20-27_JN1BJ1CW3LW375199_CAN_Messages.csv\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dbc file\n",
    "\n",
    "The dbc file stores correct labeling of CAN signals."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cantools\n",
    "from pprint import pprint\n",
    "db_toyota = cantools.database.load_file(\"data/toyota_rav4_2020.dbc\")\n",
    "db_honda = cantools.database.load_file(\"data/honda_pilot_2017.dbc\")\n",
    "db_nissan = cantools.database.load_file(\"data/nissan_leaf_2018.dbc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a dictionary to associate messages to signals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_mess2sig_dict(db):\n",
    "    dictionary = {}\n",
    "    for message in db.messages:\n",
    "        for signal in db.get_message_by_name(message.name).signals:\n",
    "            if message.name in dictionary:\n",
    "                dictionary[message.name] += [signal.name]\n",
    "            else:\n",
    "                dictionary[message.name] = [signal.name]\n",
    "    return dictionary\n",
    "\n",
    "mess2sig_toyota = get_mess2sig_dict(db_toyota)\n",
    "mess2sig_honda = get_mess2sig_dict(db_honda)\n",
    "mess2sig_nissan = get_mess2sig_dict(db_nissan)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Toyota\")\n",
    "print(mess2sig_toyota)\n",
    "print(\"Honda\")\n",
    "print(mess2sig_honda)\n",
    "print(\"Nissan\")\n",
    "print(mess2sig_nissan)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identify signals to train on\n",
    "\n",
    "Additional labeled signals can be added for training. The signals chosen for CANClassify, and their relevant Message/Signal name in the cantools database are listed below:\n",
    "\n",
    "- wheel_speed_fr, wheel_speed_fl, wheel_speed_rr, wheel_speed_rl\n",
    "\n",
    "Toyota: WHEEL_SPEEDS: ['WHEEL_SPEED_FR', 'WHEEL_SPEED_FL', 'WHEEL_SPEED_RR', 'WHEEL_SPEED_RL']\n",
    "\n",
    "Honda: 'WHEEL_SPEEDS': ['WHEEL_SPEED_FL', 'WHEEL_SPEED_FR', 'WHEEL_SPEED_RL', 'WHEEL_SPEED_RR', 'CHECKSUM']\n",
    "\n",
    "Nissan: 'WHEEL_SPEEDS_FRONT': ['WHEEL_SPEED_FR', 'WHEEL_SPEED_FL'], 'WHEEL_SPEEDS_REAR': ['WHEEL_SPEED_RR', 'WHEEL_SPEED_RL']\n",
    "\n",
    "- steer_angle, steer_angle_rate\n",
    "\n",
    "Toyota: 'STEER_ANGLE_SENSOR': ['STEER_ANGLE', 'STEER_FRACTION', 'STEER_RATE']\n",
    "\n",
    "Honda: 'STEERING_SENSORS': ['STEER_ANGLE', 'STEER_ANGLE_RATE', 'COUNTER', 'CHECKSUM']\n",
    "\n",
    "Nissan: 'STEER_ANGLE_SENSOR': ['STEER_ANGLE', 'STEER_ANGLE_RATE', 'SET_ME_X07', 'COUNTER']\n",
    "\n",
    "- brake_pedal\n",
    "\n",
    "Toyota: 'BRAKE': ['BRAKE_AMOUNT', 'BRAKE_PEDAL']\n",
    "\n",
    "Honda: 'POWERTRAIN_DATA': ['PEDAL_GAS', 'ENGINE_RPM', 'GAS_PRESSED', 'ACC_STATUS', 'BOH_17C', 'BRAKE_SWITCH', 'BOH2_17C', 'BRAKE_PRESSED', 'BOH3_17C', 'COUNTER', 'CHECKSUM']\n",
    "\n",
    "Nissan: 'BRAKE_PEDAL': ['BRAKE_PEDAL']\n",
    "\n",
    "We will directly save the binary values for each of these as {car}\\_{signal name}, which will store a list of trajectories for the specific car and signal name\n",
    "\n",
    "\n",
    "## <font color='red'> In order to change what to train on, it is necessary to change main.py's labels and label_to_messig dictionaries </font>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a list of names to refer to as vehicle identifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TOYOTA = VEHICLE('toyota_rav4_2020', db_toyota, label_to_messig_toyota, csv_paths_toyota)\n",
    "HONDA = VEHICLE('honda_pilot_2017', db_honda, label_to_messig_honda, csv_paths_honda)\n",
    "NISSAN = VEHICLE('nissan_leaf_2018', db_nissan, label_to_messig_nissan, [])\n",
    "VEHICLES = [TOYOTA, HONDA, NISSAN]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <font style='color: red'>The following cells are the cells which train the model. They may take a while and may require more intensive computing resources."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## For each csv file, collect the raw data for these signals. This takes a while. \n",
    "\n",
    "## should only be run once! Use the pickled files below to load the data again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definition: raw data\n",
    "\n",
    "Raw data refers to a mapping from a label to a list of pairs. These pairs are x and y data.\n",
    "This x and y data are directly from the can bus, so x will be an ordered collection of timestamps, and y will be an ordered collection of np boolean arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trajectories_toyota = {k: [] for k in labels}\n",
    "trajectories_honda = {k: [] for k in labels}\n",
    "# we don't have nissan data yet\n",
    "# trajectories_nissan = {k: [] for k in labels}\n",
    "\n",
    "trajectories_toyota = get_trajectory_dict(csv_paths_toyota, db_toyota, labels, \n",
    "                                          label_to_messig_toyota, more_info_to_print='toyota')\n",
    "trajectories_honda = get_trajectory_dict(csv_paths_honda, db_honda, labels, \n",
    "                                         label_to_messig_honda, more_info_to_print='honda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save and load trajectories using pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/raw_trajectories_toyota\", \"wb\") as f:\n",
    "    pickle.dump(trajectories_toyota, f)\n",
    "\n",
    "with open(\"data/raw_trajectories_honda\", \"wb\") as f:\n",
    "    pickle.dump(trajectories_honda, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/raw_trajectories_toyota\", \"rb\") as f:\n",
    "    trajectories_toyota = pickle.load(f)\n",
    "\n",
    "with open(\"data/raw_trajectories_honda\", \"rb\") as f:\n",
    "    trajectories_honda = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note: We can take a look at what is inside trajectories_toyota, which contains raw data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"The possible labels we can have inside the trajectories are: {labels}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"There are {len(trajectories_toyota[labels[0]])} '{labels[0]}' trajectories for the toyota vehicle.\")\n",
    "print(f\"The first of these has {len(trajectories_toyota[labels[0]][0][0])} timepoints.\")\n",
    "print(f\"This signal has a binary length of {len(trajectories_toyota[labels[0]][0][1].iloc[0])}.\")\n",
    "print(f\"There are {len(trajectories_honda['radar_lat'])} '{'radar_lat'}' trajectories for the honda vehicle.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 1: For each binary-valued trajectory, place it randomly into a 64-bit-long message, where the padding is random values and zeroed values\n",
    "\n",
    "There should be 8 random placements per message, and for each, pad with random values and pad with zeroed values once each."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "full_trajectories = {k: [] for k in labels}\n",
    "\n",
    "partial_trajectories = {\n",
    "    \"toyota\": trajectories_toyota, \n",
    "    \"honda\": trajectories_honda\n",
    "}\n",
    "\n",
    "for vehicle_name, car_trajectories in partial_trajectories.items():\n",
    "    for k, value_list in car_trajectories.items():\n",
    "        for xs, ys in value_list:\n",
    "            if type(xs) == pd.Series:\n",
    "                xs = xs.values\n",
    "            if type(ys) == pd.Series:\n",
    "                ys = ys.values\n",
    "            \n",
    "            print(f\"Got trajectory w/ {len(ys)} timepoints, {vehicle_name}\")\n",
    "            \n",
    "            signal_length = len(ys[0])            \n",
    "            start_positions = random.sample(range(0, 64-signal_length), 10)\n",
    "            \n",
    "            # randomized start positions for each signal\n",
    "            for start_position in start_positions:\n",
    "                \n",
    "                randomized_signal = np.array([[random.choice([False, True]) for _ in range(start_position)] + \n",
    "                                     list(yi) + \n",
    "                                     [random.choice([False, True]) for _ in range(64 - start_position - signal_length)]\n",
    "                                     for yi in ys])\n",
    "                zeroed_randomized_signal = np.array([ [False] * start_position + \n",
    "                                     list(yi) + \n",
    "                                     [False] * (64 - start_position - signal_length)\n",
    "                                     for yi in ys])\n",
    "                \n",
    "                full_trajectories[k].append( (xs, randomized_signal) )\n",
    "                full_trajectories[k].append( (xs, zeroed_randomized_signal) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for label in labels:\n",
    "    print(f\"The number of {label} trajectories is: {len(full_trajectories[label])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Given a 30MB/s write speed, pickle dumping takes about 3 minutes for a 26GB file for ~6 hours of driving"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/full_trajectories\", \"wb\") as f:\n",
    "    pickle.dump(full_trajectories, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/full_trajectories\", \"rb\") as f:\n",
    "    full_trajectories = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Algorithm 2: Interpretive Convolutions (Preprocess Data with Masked Interpretations)\n",
    "\n",
    "We have a 64 bit signal. The full_trajectories contains labels, which map to trajectories. We wish to convert these trajectories to a convolved interpretation of the signals, and make a dictionary which map from label to convolved interpretations.\n",
    "\n",
    "We have masks of size 4 (big/little unsigned), 8 (big/little signed/unsigned), 12 (big/little unsigned), 16 (big/little signed/unsigned), running across the signal and generating a single value each time.\n",
    "\n",
    "This results in:\n",
    "\n",
    "- (64 - 4 + 1) * 2 + \n",
    "- (64 - 8 + 1) * 4 +\n",
    "- (64 - 12 + 1) * 2 + \n",
    "- (64 - 12 + 1) * 4 + \n",
    "- 64 (for just the raw signal) \n",
    "\n",
    "values, that is, 716 inputs.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(64 - 4 + 1) * 2 + (64 - 8 + 1) * 4 + (64 - 12 + 1) * 2 + (64 - 16 + 1) * 4 + 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_to_convolved_interpretation = {}\n",
    "\n",
    "for k, v in full_trajectories.items():\n",
    "    new_v = []\n",
    "    count = 0\n",
    "    for _, y in v: # for each time/vals pair\n",
    "        # we don't need x, we only need y\n",
    "        \n",
    "        timestamp = datetime.strftime(datetime.now(), \"%Y/%M/%D %H:%M:%S\")\n",
    "        print(f\"{timestamp}: Interpreting key: {k}, timeseries {count+1}/{len(v)}\")\n",
    "        count += 1\n",
    "        new_values = []\n",
    "        \n",
    "        # intepretation should interpolate to a rougher timescale, 1/100\n",
    "        for i in range(0, len(y), 100): # for each 01010011 value\n",
    "            # convert from pd series if it is, else assume numpy array\n",
    "            if type(y) == pd.Series:\n",
    "                y = y.values\n",
    "                \n",
    "            # here, the length of the values should be 64 long. Ignore if not.\n",
    "            if len(y[i]) != 64:\n",
    "                print(f\"This had length {len(y[i])}, not 64. Skipping.\")\n",
    "                break\n",
    "            \n",
    "            new_values.append(convert_ith_original_signal_to_convolved_signal(y, i))\n",
    "            \n",
    "        new_values = np.array(new_values)\n",
    "        new_v.append(new_values)\n",
    "    labels_to_convolved_interpretation[k] = new_v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Center and Scale Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranges = [\n",
    "    (0, 61),\n",
    "    (61, 122),\n",
    "    (122, 179),\n",
    "    (179, 236),\n",
    "    (236, 293),\n",
    "    (293, 350),\n",
    "    (350, 403),\n",
    "    (403, 456),\n",
    "    (456, 505),\n",
    "    (505, 554),\n",
    "    (554, 603),\n",
    "    (603, 652),\n",
    "    (652, 716)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "means = []\n",
    "\n",
    "variances = []\n",
    "\n",
    "# independent variances and means should be calculated for each of these ranges\n",
    "\n",
    "for r in ranges:\n",
    "    print(f\"Processing range {r}\")\n",
    "    collected_range = []\n",
    "    for _, list_of_new_vals in labels_to_convolved_interpretation.items():\n",
    "        for y in list_of_new_vals:\n",
    "            collected_range.extend([y_i[r[0]:r[1]] for y_i in y])\n",
    "    means.append(np.mean(np.array(collected_range), axis=0))\n",
    "    variances.append(np.var(np.array(collected_range), axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/data_means\", \"wb\") as f:\n",
    "    pickle.dump(means, f)\n",
    "\n",
    "with open(\"data/data_vars\", \"wb\") as f:\n",
    "    pickle.dump(variances, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/data_means\", \"rb\") as f:\n",
    "    means = pickle.load(f)\n",
    "\n",
    "with open(\"data/data_vars\", \"rb\") as f:\n",
    "    variances = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# center and mean data\n",
    "for i in range(len(ranges)):\n",
    "    print(f\"Centering and scaling range {ranges[i]}\")\n",
    "    for _, list_of_new_vals in labels_to_convolved_interpretation.items():\n",
    "        for y in list_of_new_vals:\n",
    "            for j in range(len(y)):\n",
    "                y[j][ranges[i][0]:ranges[i][1]] -= means[i]\n",
    "                y[j][ranges[i][0]:ranges[i][1]] /= variances[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/labels_to_convolved_interpretation\", \"wb\") as f:\n",
    "    pickle.dump(labels_to_convolved_interpretation, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/labels_to_convolved_interpretation\", \"rb\") as f:\n",
    "    labels_to_convolved_interpretation = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finalizing the data: making X/Y arrays\n",
    "\n",
    "Convert data into X and Y arrays before train/test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "labels_to_indices = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "def generate_label_vector_from_label(input_label):\n",
    "    vector = np.zeros(len(labels))\n",
    "    vector[labels_to_indices[input_label]] = 1\n",
    "    return vector\n",
    "\n",
    "for label, list_of_new_vals in labels_to_convolved_interpretation.items():\n",
    "    y_vector = generate_label_vector_from_label(label)\n",
    "    for y in list_of_new_vals:\n",
    "        # grab data in large chunks, 100 each. \n",
    "        # 100 timesteps is a good amount of time for the detection of patterns in a signal. Longer also works, but \n",
    "        # shorter, means there are not enough patterns for the LSTM to pick up on\n",
    "        for i in range(0, len(y) - 100, 100):\n",
    "            X.append(np.array(y[i:i+100]))\n",
    "            Y.append(y_vector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"data/data_x\", \"wb\") as f:\n",
    "    pickle.dump(X, f)\n",
    "\n",
    "with open(\"data/data_y\", \"wb\") as f:\n",
    "    pickle.dump(Y, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}